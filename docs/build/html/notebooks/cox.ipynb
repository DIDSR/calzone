{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# COX calibration analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Theoretical Background\n",
    "\n",
    "Cox calibration analysis is both a logistic recalibration technique and a method to examine the current calibration of a model. To perform the analysis, we first need to fit a new logistic regression model using logit(log odds, aka $\\log\\frac{\\hat{p}}{1-\\hat{p}}$) as the predictor variable and the outcome the predictor variable. \n",
    "\n",
    "$$\n",
    "p_{new} = \\frac{1}{1+e^{-(a + b \\cdot \\log\\frac{\\hat{p}}{1-\\hat{p}})}}\n",
    "$$\n",
    "\n",
    "In the case of perfect calibration, $P(Y=1|p=\\hat{p}) = \\hat{p}$ and the new probability $p_{new}$ is equal to the original probability $\\hat{p}$. That means $a=0$ and $b=1$. If $b>1$, the model is over-confidence at high probabilities and under-confidence at low probabilities. If $b<1$, the model is under-confidence at high probabilities and over-confidence at low probabilities. If $a>0$, the model is over-confidence at all probabilities. If $a<0$, the model is under-confidence at all probabilities.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
